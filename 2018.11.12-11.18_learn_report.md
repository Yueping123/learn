1、用Classifier_BiLSTM_Attention.py跑bilstem，
'''
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 80)                0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 80, 300)           2769900   
_________________________________________________________________
bidirectional_1 (Bidirection (None, 80, 200)           320800    
_________________________________________________________________
attention_layer_1 (Attention (None, 200)               40200     
_________________________________________________________________
dense_1 (Dense)              (None, 100)               20100     
_________________________________________________________________
dense_2 (Dense)              (None, 2)                 202       
=================================================================
Total params: 3,151,202
Trainable params: 381,302
Non-trainable params: 2,769,900
_________________________________________________________________
Classifier_BiLSTM_Attention.py:301: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  model.fit(x_train, y_train, validation_data=(x_val, y_val),nb_epoch=20, batch_size=128,class_weight='auto')
Train on 6443 samples, validate on 1610 samples
Epoch 1/20
2018-11-18 21:54:21.543275: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
6443/6443 [==============================] - 18s 3ms/step - loss: 18.3959 - acc: 0.7427 - val_loss: 17.6917 - val_acc: 0.7571
Epoch 2/20
6443/6443 [==============================] - 15s 2ms/step - loss: 18.2684 - acc: 0.7523 - val_loss: 17.8222 - val_acc: 0.7571
Epoch 3/20
6443/6443 [==============================] - 15s 2ms/step - loss: 18.1756 - acc: 0.7520 - val_loss: 17.5609 - val_acc: 0.7565
Epoch 4/20
6443/6443 [==============================] - 15s 2ms/step - loss: 17.9896 - acc: 0.7517 - val_loss: 17.2984 - val_acc: 0.7565
Epoch 5/20
6443/6443 [==============================] - 16s 2ms/step - loss: 16.9014 - acc: 0.7636 - val_loss: 15.7650 - val_acc: 0.7863
Epoch 6/20
6443/6443 [==============================] - 15s 2ms/step - loss: 15.7237 - acc: 0.7947 - val_loss: 14.7222 - val_acc: 0.8124
Epoch 7/20
6443/6443 [==============================] - 16s 2ms/step - loss: 15.0104 - acc: 0.8080 - val_loss: 14.5757 - val_acc: 0.8186
Epoch 8/20
6443/6443 [==============================] - 16s 2ms/step - loss: 14.5339 - acc: 0.8141 - val_loss: 14.1154 - val_acc: 0.8242
Epoch 9/20
6443/6443 [==============================] - 15s 2ms/step - loss: 14.2848 - acc: 0.8217 - val_loss: 14.2900 - val_acc: 0.8248
Epoch 10/20
6443/6443 [==============================] - 15s 2ms/step - loss: 13.7303 - acc: 0.8280 - val_loss: 16.8606 - val_acc: 0.7913
Epoch 11/20
6443/6443 [==============================] - 16s 2ms/step - loss: 13.4298 - acc: 0.8341 - val_loss: 13.6264 - val_acc: 0.8311
Epoch 12/20
6443/6443 [==============================] - 16s 2ms/step - loss: 13.1053 - acc: 0.8380 - val_loss: 13.8512 - val_acc: 0.8199
Epoch 13/20
6443/6443 [==============================] - 16s 2ms/step - loss: 12.6736 - acc: 0.8428 - val_loss: 13.9464 - val_acc: 0.8304
Epoch 14/20
6443/6443 [==============================] - 16s 2ms/step - loss: 12.4148 - acc: 0.8505 - val_loss: 13.4283 - val_acc: 0.8342
Epoch 15/20
6443/6443 [==============================] - 15s 2ms/step - loss: 12.1357 - acc: 0.8504 - val_loss: 13.9005 - val_acc: 0.8323
Epoch 16/20
6443/6443 [==============================] - 15s 2ms/step - loss: 11.9341 - acc: 0.8552 - val_loss: 13.6604 - val_acc: 0.8335
Epoch 17/20
6443/6443 [==============================] - 15s 2ms/step - loss: 11.5181 - acc: 0.8614 - val_loss: 13.5271 - val_acc: 0.8348
Epoch 18/20
6443/6443 [==============================] - 15s 2ms/step - loss: 11.3210 - acc: 0.8636 - val_loss: 13.5888 - val_acc: 0.8292
Epoch 19/20
6443/6443 [==============================] - 16s 2ms/step - loss: 11.0711 - acc: 0.8692 - val_loss: 14.5344 - val_acc: 0.8205
Epoch 20/20
6443/6443 [==============================] - 15s 2ms/step - loss: 10.7640 - acc: 0.8688 - val_loss: 13.7737 - val_acc: 0.8304
[0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0
 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1
 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0
 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1
 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0
 1 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0
 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0
 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1
 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1
 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1
 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0
 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 1 0
 1 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 0
 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0
 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0
 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1]
 
 '''
2、对于数据集不均衡的问题，感觉下列方法效果就acc的值来看，没有多大改变
（1）采用在设置权重值classweight运行
cw=[0:1,1:3]
#model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy']，class_weight=cw)

（2）重新定义loss函数，令
model.compile(optimizer='rmsprop', loss=[focal_loss([6066,1987])], metrics=['accuracy'])
'''
def focal_loss(classes_num, gamma=2., alpha=.25, e=0.1):
    # classes_num contains sample number of each classes
    def focal_loss_fixed(target_tensor, prediction_tensor):
        
       # prediction_tensor is the output tensor with shape [None, 100], where 100 is the number of classes
       # target_tensor is the label tensor, same shape as predcition_tensor
        
        import tensorflow as tf
        from tensorflow.python.ops import array_ops
        from keras import backend as K

        #1# get focal loss with no balanced weight which presented in paper function (4)
        zeros = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)
        one_minus_p = array_ops.where(target_tensor > zeros, target_tensor - prediction_tensor, zeros)
        FT = -1 * (one_minus_p ** gamma) * tf.log(tf.clip_by_value(prediction_tensor, 1e-8, 1.0))

        #2# get balanced weight alpha
        classes_weight = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)

        total_num = float(sum(classes_num))
        classes_w_t1 = [ total_num / ff for ff in classes_num ]
        sum_ = sum(classes_w_t1)
        classes_w_t2 = [ ff/sum_ for ff in classes_w_t1 ]   #scale
        classes_w_tensor = tf.convert_to_tensor(classes_w_t2, dtype=prediction_tensor.dtype)
        classes_weight += classes_w_tensor

        alpha = array_ops.where(target_tensor > zeros, classes_weight, zeros)

        #3# get balanced focal loss
        balanced_fl = alpha * FT
        balanced_fl = tf.reduce_sum(balanced_fl)

        #4# add other op to prevent overfit
        # reference : https://spaces.ac.cn/archives/4493
        nb_classes = len(classes_num)
        fianal_loss = (1-e) * balanced_fl + e * K.categorical_crossentropy(K.ones_like(prediction_tensor)/nb_classes, prediction_tensor)

        return fianal_loss
    return focal_loss_fixed
 '''
 
 (3)尝试用adaboost
 
