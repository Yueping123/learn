
1、定义f1函数,eartlystopping中monitor=f1,加入到callbacks.

```
def f1(y_true, y_pred):
    def recall(y_true, y_pred):
        """Recall metric.
        Only computes a batch-wise average of recall.
        Computes the recall, a metric for multi-label classification of
        how many relevant items are selected.
        """
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall
 
    def precision(y_true, y_pred):
        """Precision metric.
        Only computes a batch-wise average of precision.
        Computes the precision, a metric for multi-label classification of
        how many selected items are relevant.
        """
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision
    precision = precision(y_true, y_pred)
    recall = recall(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))
    
    
 
   model.compile(optimizer='rmsprop', loss=[focal_loss([6066,1987])], metrics=[f1])

   es = EarlyStopping(monitor='val_f1', patience=20)
   mc = ModelCheckpoint(filepath,monitor='val_f1', save_best_only=True)
   tb = TensorBoard(log_dir=log_dir, histogram_freq=0)

   early_stopping = EarlyStopping(monitor='val_f1', patience=50, verbose=2)
   
   history=model.fit(x_train,y_train, validation_data=[x_val, y_val], batch_size=batch_size, epochs=num_epochs, verbose=2,callbacks=[metrics,early_stopping,mc,tb])

```
2、集成cnn与lstm模型采用average方法
```

#集成模型
cnn_model =cnn(sequence_input)

lstm_model =LSTM_Attention(sequence_input)

cnn_model.load_weights(filepath_cnn)

lstm_model.load_weights(filepath_lstm)

models =[cnn_model,lstm_model]

#集成模型的定义是很直接的。它使用了所有模型共享的输入层。在顶部的层中，该集成通过使用 Average() 合并层计算三个模型输出的平均值。

def ensemble(models,sequence_input):

  outputs =[model.outputs[0] for model in models]

  y =Average()(outputs)

  model =Model(sequence_input,y,name='ensemble')

  return model
```
3、针对不平衡数据采用过抽样的方法，将lable标签少的样本进行复制，是类别样本达到大约1：1。


4、使用之定义的focal_loss函数，
上述2种方法运行后的结果提交test的label预测均为0.67左右。

```
Here are your submissions to date ( indicates submission on leaderboard ):

#	SCORE	FILENAME	SUBMISSION DATE	STATUS	
1	---	submission.zip	11/23/2018 14:23:29	Submitted		
2	0.6685006878	submission.zip	11/24/2018 06:11:48	Finished		
3	0.6685006878	submission.zip	11/24/2018 06:11:49	Finished		
4	0.2359550562	submission.zip	11/24/2018 07:19:53	Finished		
5	0.6693227092	submission.zip	11/24/2018 08:06:59	Finished		
6	0.6693227092	submission.zip	11/24/2018 08:08:47	Finished		
7	0.4944649446	submission.zip	11/24/2018 13:42:55	Failed		
8	0.4944649446	submission.zip	11/24/2018 13:43:53	Finished		
9	0.6737089202	submission.zip	11/24/2018 14:35:32	Finished		
```
5、运行的部分log

```
cnn：
Epoch 45/100
 - 3s - loss: 0.3582 - f1: 0.9930 - val_loss: 10.8339 - val_f1: 0.7994
 — val_f1: 0.7461944789746457
Epoch 46/100
 - 3s - loss: 0.3822 - f1: 0.9919 - val_loss: 11.1262 - val_f1: 0.7919
 — val_f1: 0.7389999879021039
Epoch 47/100
 - 2s - loss: 0.3385 - f1: 0.9929 - val_loss: 11.6552 - val_f1: 0.7988
 — val_f1: 0.7477470158477778
Epoch 48/100
 - 3s - loss: 0.4015 - f1: 0.9918 - val_loss: 10.7576 - val_f1: 0.7919
 — val_f1: 0.7446079292525847
Epoch 49/100
 - 3s - loss: 0.3699 - f1: 0.9919 - val_loss: 12.1346 - val_f1: 0.8168
 — val_f1: 0.7540560428060032
Epoch 50/100
 - 3s - loss: 0.3572 - f1: 0.9924 - val_loss: 11.7582 - val_f1: 0.7957
 — val_f1: 0.7411068246546031
Epoch 51/100
 - 3s - loss: 0.3561 - f1: 0.9922 - val_loss: 11.6006 - val_f1: 0.7919
 — val_f1: 0.7422154416463417
Epoch 00051: early stopping
******************************lstm_model
Tensor("attention_layer_1/truediv:0", shape=(?, 80, 200), dtype=float32)
Tensor("bidirectional_1/concat:0", shape=(?, ?, 200), dtype=float32)
Tensor("attention_layer_1/mul:0", shape=(?, 80, 200), dtype=float32)

lstm：

Epoch 45/100
 - 3s - loss: 0.3582 - f1: 0.9930 - val_loss: 10.8339 - val_f1: 0.7994
 — val_f1: 0.7461944789746457
Epoch 46/100
 - 3s - loss: 0.3822 - f1: 0.9919 - val_loss: 11.1262 - val_f1: 0.7919
 — val_f1: 0.7389999879021039
Epoch 47/100
 - 2s - loss: 0.3385 - f1: 0.9929 - val_loss: 11.6552 - val_f1: 0.7988
 — val_f1: 0.7477470158477778
Epoch 48/100
 - 3s - loss: 0.4015 - f1: 0.9918 - val_loss: 10.7576 - val_f1: 0.7919
 — val_f1: 0.7446079292525847
Epoch 49/100
 - 3s - loss: 0.3699 - f1: 0.9919 - val_loss: 12.1346 - val_f1: 0.8168
 — val_f1: 0.7540560428060032
Epoch 50/100
 - 3s - loss: 0.3572 - f1: 0.9924 - val_loss: 11.7582 - val_f1: 0.7957
 — val_f1: 0.7411068246546031
Epoch 51/100
 - 3s - loss: 0.3561 - f1: 0.9922 - val_loss: 11.6006 - val_f1: 0.7919
 — val_f1: 0.7422154416463417
Epoch 00051: early stopping
******************************lstm_model
Tensor("attention_layer_1/truediv:0", shape=(?, 80, 200), dtype=float32)
Tensor("bidirectional_1/concat:0", shape=(?, ?, 200), dtype=float32)
Tensor("attention_layer_1/mul:0", shape=(?, 80, 200), dtype=float32)

```
6、感觉在训练集上训练的还可以，在test上效果很差，逐步改善参数。






